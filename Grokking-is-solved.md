# Grokking is (more or less) solved

The most interesting conceptual AI advances in recent months seem to come from "prosaic alignment" start-ups. 
These are companies which believe that the current trend of improving Transformer models is fairly likely to lead straight to AGI, 
and that better understanding of the nature and properties of these models is key to AI safety (and, of course, it's also key to better AI capabilities).

And it is often the case that the key elements of work are done by people "on the edge", "in the penumbra" of those alignment start-ups.

---

Recently the key new understanding of large Transformer models as simulators has emerged. 
That work has been done "while at Conjecture", but is not listed as directly coming from Conjecture 
(which is one of the "prosaic alignment" start-ups). 

I believe that the key people involved are still at Conjecture, but it looks like they feel it's
appropriate to keep some distance between Conjecture and this work. 

I am continuing to take notes of those materials here: [Generative autoregressive models are similators](../../tree/main/Generative-autoregressive-models-are-similators)

---

Here is another one of those stories. Grokking is a phenomenon, where small Transformers look at a part of a mathematical structure for quite a while, 
and then rather suddenly transition to understanding the whole of that mathematical structure including the part they never see in training. 
It has been discovered in 2021 and has been a subject of a number of follow-up attempts to understand it. Here is my list of
Grokking-related literature: 

The recent breakthrough has been done in mid-August by Neel Nanda who left Anthropic (perhaps the most famous of the "prosaic alignment" start-ups) a few months ago. 
And it looks like he has more or less solved the mysteries behind this phenomenon. Here are the relevant links:
