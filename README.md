# 2022 design and research notes

This is a continuation of

https://github.com/anhinga/2021-notes

## My 2022 updates

April 2022: Modern differentiable programming frameworks are capable of taking gradients with respect to variables stored inside nested
dictionaries:

  * JAX: https://github.com/anhinga/jax-pytree-example
  
  * Zygote.jl (Julia Flux): https://github.com/anhinga/julia-flux-drafts/tree/main/arxiv-1606-09470-section3
  
June 2022: First successful experiments in DMM training and in program synthesis/circuit synthesis/DMM synthesis 
via neural architecture search using Zygote.jl. 
The synthesized DMMs exhibit pretty impressive generalization properties:

https://github.com/anhinga/DMM-synthesis-lab-journal/blob/main/history.md

## Transformer understanding breakthroughs in 2022 which seem to be particularly important

September 2022: [Generative autoregressive models are similators](Generative-autoregressive-models-are-similators)

