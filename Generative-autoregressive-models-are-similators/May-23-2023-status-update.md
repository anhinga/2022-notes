# Status update on Janus' Simulator theory

1. There are two relevant sequences on LessWrong:

     * [Simulators](https://www.lesswrong.com/s/N7nDePaNabJdnbXeE)
  
     * [Cyborgism](https://www.lesswrong.com/s/f2YA4eGskeztcJsqT)
  
2. There is a good deal of disagreement on the status of the Simulator theory

     * People generally believe that it is a great deconfusion tool.
  
     * But they disagree on whether it is literally true, or just a rather imprecise approximation
  
[The Compleat Cybornaut](https://www.lesswrong.com/posts/iFBdEqEogtXcjCPBB/the-compleat-cybornaut) has a particularly good discussion of that.

I am going to quote from the **Why these frames?** section:

> There's a `fair amount` of `discourse` surrounding the analogies given above - whether they describe these systems with high enough precision, the kinds of object-level claims they make, where they break, and so on. `Some` have `written` about how GPTs are better viewed simply as predictors, for instance, because it seemingly makes fewer questionable prescriptive claims on the model (note DragonGod's `comment` on one of the linked posts, or Janus' succinct summary `reply`, on a comparison between the two frames).
>
> The purpose of frames such as simulators or semiotic physics isn't, however, to make any differing object-level claims about the properties or nature of these models. Their sole utility is in being _evocative_.
> 
> GPTs are hilariously high-dimensional in their mechanics, and we lack any concrete method to speak about them at high levels of fidelity, having neither satisfactory formal understanding nor sufficient interpretability. Our choice is either to pick a very grounded frame with high precision - for instance, you could simply form deep, sophisticated views on their properties by simply viewing them through the way transformers work generally or as next-token predictors - or a frame that attempts to speak to high-level properties, such as simulators.
>
> The key factor here isn't which of them is ultimately _correct_ - neither can really model these high-dimensional structures strictly accurately at a conceptual level - but which of them evokes higher-quality predictions about the properties of those structures from `within a human mind`. Certainly, it's likely that you can ultimately converge onto the same predictions from within any frame, but the utility of a frame is in which of them makes those predictions more accessible.
>
> Almost by definition, this is a subjective thing - it's entirely plausible that viewing GPTs as predictors or `probabilistic constraint satisfaction problem solvers` makes high-level properties more intuitive to you than viewing them as simulators would have. There's likely some shared threshold here - it seems less likely a priori that thinking purely about the transformer mechanism would allow you to predict the properties of these models as they scale or how they interact with fine-tuning/RLHF before other frames, for instance - but ultimately the value of frames, the value of simulators or semiotic physics, is in how evocative they are to _you_. There are no normative claims here, except maybe that there's a lot of value in frames which aren't built from the bottom-up of the models as long as they're still evocative.
  
