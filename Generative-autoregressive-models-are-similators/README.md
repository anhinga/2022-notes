# Generative autoregressive models are similators

A radical novel understanding of the nature of GPT-3-like models seems to be crystallizing.

_These models are simulators, they generate various entities (which tend to be rather short-lived at the moment),
and users are interacting with those entities. This novel understanding explains a lot of observable phenomena and
opens new promising directions of work._

A lot of people were seeing glimpses of this for a few years now. E.g. when I was playing with GPT-2 in 2019, I noticed that
it looked like a virtual personality was created at first (sampled from a distribution of virtual personalities
conditioned by the initial prompt), and then that personality was speaking to me. Or, at least, that was my impression.

---

Fast-forwarding to September 2022:

The following text has been published by **"janus"** on Sep 2, 2022:

https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators

It has been further publicized and discussed by Scott Alexander on Sep 19:

https://astralcodexten.substack.com/p/janus-gpt-wrangling

This both crystallizes a lot of that new understanding, shows that this viewpoint is gradually proliferating in leading
organizations focusing on better understanding the true nature of modern generative Transformers and on leveraging this
improved understanding, and helps to disseminate this viewpoint (due to the popularity of Scott Alexander's substack in
the relevant circles of professionals).

This subdirectory is for the materials related to this new understanding.

---

### My reading notes:

[reading-janus-gpt-wrangling.md](reading-janus-gpt-wrangling.md) - written (this page also contains links to some tools and papers by "Janus" including her **loom** tool and "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm" paper, https://arxiv.org/abs/2102.07350)

[reading-simulators-essay.md](reading-simulators-essay.md) - written

---

### Thinking further about this topic

_Work in progress_
